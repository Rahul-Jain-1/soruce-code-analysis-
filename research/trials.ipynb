{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in /config/.local/lib/python3.8/site-packages (3.0.3)\n",
      "Requirement already satisfied: GitPython in /config/.local/lib/python3.8/site-packages (3.1.43)\n",
      "Requirement already satisfied: python-dotenv in /config/.local/lib/python3.8/site-packages (1.0.1)\n",
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 38.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chromadb==0.4.4\n",
      "  Downloading chromadb-0.4.4-py3-none-any.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 60.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langchain==0.0.249\n",
      "  Downloading langchain-0.0.249-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 50.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: blinker>=1.6.2 in /config/.local/lib/python3.8/site-packages (from flask) (1.7.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /config/.local/lib/python3.8/site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0; python_version < \"3.10\" in /config/.local/lib/python3.8/site-packages (from flask) (7.1.0)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /config/.local/lib/python3.8/site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /config/.local/lib/python3.8/site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /config/.local/lib/python3.8/site-packages (from flask) (3.0.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /config/.local/lib/python3.8/site-packages (from GitPython) (4.0.11)\n",
      "Collecting requests>=2.20\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 54.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 9.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
      "\u001b[K     |████████████████████████████████| 777 kB 59.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers>=0.13.2\n",
      "  Downloading tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 54.8 MB/s eta 0:00:01\n",
      "\u001b[?25hProcessing /config/.cache/pip/wheels/54/4a/f8/2803c6041841502d0d21fb2a62d401d14716dfeb2261a3a70b/PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Collecting graphlib-backport>=1.0.3; python_version < \"3.9\"\n",
      "  Downloading graphlib_backport-1.1.0-py3-none-any.whl (7.1 kB)\n",
      "Collecting pydantic<2.0,>=1.9\n",
      "  Downloading pydantic-1.10.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 59.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.21.6\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 57.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /config/.local/lib/python3.8/site-packages (from chromadb==0.4.4) (4.11.0)\n",
      "Collecting pulsar-client>=3.1.0\n",
      "  Downloading pulsar_client-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 56.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.16.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 26.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting overrides>=7.3.1\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting posthog>=2.4.0\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 484 kB/s  eta 0:00:01\n",
      "\u001b[?25hProcessing /config/.cache/pip/wheels/86/89/93/3fb39e34dc50ccb920efb7c6f2b6971feab5910f46b1f4dda6/chroma_hnswlib-0.7.2-cp38-cp38-linux_x86_64.whl\n",
      "Collecting fastapi<0.100.0,>=0.95.2\n",
      "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 8.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Downloading numexpr-2.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[K     |████████████████████████████████| 384 kB 63.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.29-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 59.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML>=5.4.1\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "\u001b[K     |████████████████████████████████| 736 kB 62.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\"\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.11\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /config/.local/lib/python3.8/site-packages (from importlib-metadata>=3.6.0; python_version < \"3.10\"->flask) (3.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /config/.local/lib/python3.8/site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /config/.local/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.1)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 59.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 64.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 8.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 65.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 64.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 64.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 65.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface_hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[K     |████████████████████████████████| 388 kB 65.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 59.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /config/.local/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.4) (24.0)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 67.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 9.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting websockets>=10.4; extra == \"standard\"\n",
      "  Downloading websockets-12.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 64.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httptools>=0.5.0; extra == \"standard\"\n",
      "  Downloading httptools-0.6.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)\n",
      "\u001b[K     |████████████████████████████████| 354 kB 66.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0; (sys_platform != \"win32\" and (sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\")) and extra == \"standard\"\n",
      "  Downloading uvloop-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 53.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting watchfiles>=0.13; extra == \"standard\"\n",
      "  Downloading watchfiles-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 60.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /config/.local/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.4.4) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six>=1.5 in /config/.local/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.4.4) (1.16.0)\n",
      "Collecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))\n",
      "  Downloading greenlet-3.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (667 kB)\n",
      "\u001b[K     |████████████████████████████████| 667 kB 50.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[K     |████████████████████████████████| 171 kB 64.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 60.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 9.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting anyio>=3.0.0\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting exceptiongroup>=1.0.2; python_version < \"3.11\"\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: charset-normalizer, urllib3, idna, certifi, requests, multidict, frozenlist, aiosignal, attrs, yarl, async-timeout, aiohttp, tqdm, openai, regex, tiktoken, PyYAML, fsspec, filelock, huggingface-hub, tokenizers, pypika, graphlib-backport, pydantic, numpy, pulsar-client, flatbuffers, mpmath, sympy, humanfriendly, coloredlogs, protobuf, onnxruntime, overrides, h11, websockets, httptools, uvloop, exceptiongroup, sniffio, anyio, watchfiles, uvicorn, monotonic, backoff, posthog, chroma-hnswlib, starlette, fastapi, importlib-resources, chromadb, numexpr, mypy-extensions, typing-inspect, marshmallow, dataclasses-json, greenlet, SQLAlchemy, tenacity, langsmith, openapi-schema-pydantic, langchain\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script openai is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script humanfriendly is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script coloredlogs is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script onnxruntime_test is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script watchfiles is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script uvicorn is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script langsmith is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script langchain-server is installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.29 aiohttp-3.9.4 aiosignal-1.3.1 anyio-4.3.0 async-timeout-4.0.3 attrs-23.2.0 backoff-2.2.1 certifi-2024.2.2 charset-normalizer-3.3.2 chroma-hnswlib-0.7.2 chromadb-0.4.4 coloredlogs-15.0.1 dataclasses-json-0.5.14 exceptiongroup-1.2.0 fastapi-0.99.1 filelock-3.13.4 flatbuffers-24.3.25 frozenlist-1.4.1 fsspec-2024.3.1 graphlib-backport-1.1.0 greenlet-3.0.3 h11-0.14.0 httptools-0.6.1 huggingface-hub-0.22.2 humanfriendly-10.0 idna-3.7 importlib-resources-6.4.0 langchain-0.0.249 langsmith-0.0.92 marshmallow-3.21.1 monotonic-1.6 mpmath-1.3.0 multidict-6.0.5 mypy-extensions-1.0.0 numexpr-2.8.6 numpy-1.24.4 onnxruntime-1.16.3 openai-0.28.0 openapi-schema-pydantic-1.2.4 overrides-7.7.0 posthog-3.5.0 protobuf-5.26.1 pulsar-client-3.5.0 pydantic-1.10.15 pypika-0.48.9 regex-2023.12.25 requests-2.31.0 sniffio-1.3.1 starlette-0.27.0 sympy-1.12 tenacity-8.2.3 tiktoken-0.6.0 tokenizers-0.15.2 tqdm-4.66.2 typing-inspect-0.9.0 urllib3-2.2.1 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask GitPython python-dotenv openai==0.28 tiktoken chromadb==0.4.4 langchain==0.0.249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/config/workspace/research'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo '/config/workspace/research/test_repo1/.git'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path=\"test_repo1/\"\n",
    "Repo.clone_from(\"https://github.com/Rahul-Jain-1/US-VISA-APPROVAL\",to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path=\"test_repo1/\"\n",
    "loader = GenericLoader.from_filesystem(repo_path+'/us_visa',\n",
    "                                        glob=\"**/*\",\n",
    "                                        suffixes=[\".py\"],\n",
    "                                        parser=LanguageParser(language=Language.PYTHON,parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': 'test_repo1/us_visa/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# this file is used to tell what error come at which line no and file name\\nimport os\\nimport sys\\n\\ndef error_message_detail(error, error_detail:sys):\\n    _, _, exc_tb = error_detail.exc_info()\\n    file_name = exc_tb.tb_frame.f_code.co_filename\\n    error_message = \"Error occurred python script name [{0}] line number [{1}] error message [{2}]\".format(\\n        file_name, exc_tb.tb_lineno, str(error)\\n    )\\n\\n    return error_message\\n\\nclass USvisaException(Exception):\\n    def __init__(self, error_message, error_detail):\\n        \"\"\"\\n        :param error_message: error message in string format\\n        \"\"\"\\n        super().__init__(error_message)\\n        self.error_message = error_message_detail(\\n            error_message, error_detail=error_detail\\n        )\\n\\n    def __str__(self):\\n        return self.error_message', metadata={'source': 'test_repo1/us_visa/exception/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os \\nfrom datetime import date\\n\\nDATABASE_NAME = \"US_VISA\"\\nCOLLECTION_NAME=\"visa_data\"\\nMONGODB_URL_KEY=\"\"\\n\\nPIPELINE_NAME: str=\"usvisa\"\\nARTIFACT_DIR: str=\"artifact\"\\n\\nMODEL_FILE_NAME = \"model.pkl\"\\n\\nTARGET_COLUMN = \"case_status\"\\nCURRENT_YEAR = date.today().year\\nPROCESSING_OBJECT_FILE_NAME =\"processing.pkl\"\\n\\nFILE_NAME: str = \"usvisa.csv\"\\nTRAIN_FILE_NAME: str = \"train.csv\"\\nTEST_FILE_NAME: str=\"test.csv\"\\n\\n\\n\"\"\"\\nData ingestion related constant start with DATA_INGESTION VAR NAME\\n\"\"\" \\nDATA_INGESTION_COLLECTION_NAME: str= \"visa_data\"\\nDATA_INGESTION_DIR_NAME: str =\"data_ingestion\"\\nDATA_INGESTION_FEATURE_STORE_DIR: str=\"feature_store\"\\nDATA_INGESTION_INGESTED_DIR: str=\"ingested\"\\nDATA_INGESTION_TRAIN_TEST_SPLIT_RATIO: float =0.2 ', metadata={'source': 'test_repo1/us_visa/constants/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/data_access/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from us_visa.configuration.mongo_db_connection import MongoDBClient\\nfrom us_visa.constants import DATABASE_NAME\\nfrom us_visa.exception import USvisaException\\nimport pandas as pd\\nimport sys\\nfrom typing import Optional\\nimport numpy as np\\n\\n\\n\\nclass USvisaData:\\n    \"\"\"\\n    This class help to export entire mongo db record as pandas dataframe\\n    \"\"\"\\n\\n    def __init__(self):\\n        \"\"\"\\n        \"\"\"\\n        try:\\n            self.mongo_client = MongoDBClient(database_name=DATABASE_NAME)\\n        except Exception as e:\\n            raise USvisaException(e,sys)\\n        \\n\\n    def export_collection_as_dataframe(self,collection_name:str,database_name:Optional[str]=None)->pd.DataFrame:\\n        try:\\n            \"\"\"\\n            export entire collectin as dataframe:\\n            return pd.DataFrame of collection\\n            \"\"\"\\n            if database_name is None:\\n                collection = self.mongo_client.database[collection_name]\\n            else:\\n                collection = self.mongo_client[database_name][collection_name]\\n\\n            df = pd.DataFrame(list(collection.find()))\\n            if \"_id\" in df.columns.to_list():\\n                df = df.drop(columns=[\"_id\"], axis=1)\\n            df.replace({\"na\":np.nan},inplace=True)\\n            return df\\n        except Exception as e:\\n            raise USvisaException(e,sys)', metadata={'source': 'test_repo1/us_visa/data_access/usvisa_data.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/components/model_pusher.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\nfrom pandas import DataFrame\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom us_visa.entity.config_entity import DataIngestionConfig\\nfrom us_visa.entity.artifact_entity import DataIngestionArtifact\\nfrom us_visa.exception import USvisaException\\nfrom us_visa.logger import logging\\nfrom us_visa.data_access.usvisa_data import USvisaData\\n\\nclass DataIngestion:\\n    def __init__(self,data_ingestion_config:DataIngestionConfig=DataIngestionConfig()):\\n        \"\"\"\\n        :param data_ingestion_config: configuration for data ingestion\\n        \"\"\"\\n        try:\\n            self.data_ingestion_config = data_ingestion_config\\n        except Exception as e:\\n            raise USvisaException(e,sys)\\n\\n    def export_data_into_feature_store(self)->DataFrame:\\n        \"\"\"\\n        Method Name :   export_data_into_feature_store\\n        Description :   This method exports data from mongodb to csv file\\n        \\n        Output      :   data is returned as artifact of data ingestion components\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(f\"Exporting data from mongodb\")\\n            usvisa_data = USvisaData()\\n            dataframe = usvisa_data.export_collection_as_dataframe(collection_name=\\n                                                                   self.data_ingestion_config.collection_name)\\n            logging.info(f\"Shape of dataframe: {dataframe.shape}\")\\n            feature_store_file_path  = self.data_ingestion_config.feature_store_file_path\\n            dir_path = os.path.dirname(feature_store_file_path)\\n            os.makedirs(dir_path,exist_ok=True)\\n            logging.info(f\"Saving exported data into feature store file path: {feature_store_file_path}\")\\n            dataframe.to_csv(feature_store_file_path,index=False,header=True)\\n            return dataframe\\n\\n        except Exception as e:\\n            raise USvisaException(e,sys)\\n\\n    def split_data_as_train_test(self,dataframe: DataFrame) ->None:\\n        \"\"\"\\n        Method Name :   split_data_as_train_test\\n        Description :   This method splits the dataframe into train set and test set based on split ratio \\n        \\n        Output      :   Folder is created in s3 bucket\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered split_data_as_train_test method of Data_Ingestion class\")\\n\\n        try:\\n            train_set, test_set = train_test_split(dataframe, test_size=self.data_ingestion_config.train_test_split_ratio)\\n            logging.info(\"Performed train test split on the dataframe\")\\n            logging.info(\\n                \"Exited split_data_as_train_test method of Data_Ingestion class\"\\n            )\\n            dir_path = os.path.dirname(self.data_ingestion_config.training_file_path)\\n            os.makedirs(dir_path,exist_ok=True)\\n            \\n            logging.info(f\"Exporting train and test file path.\")\\n            train_set.to_csv(self.data_ingestion_config.training_file_path,index=False,header=True)\\n            test_set.to_csv(self.data_ingestion_config.testing_file_path,index=False,header=True)\\n\\n            logging.info(f\"Exported train and test file path.\")\\n        except Exception as e:\\n            raise USvisaException(e, sys) from e\\n\\n    def initiate_data_ingestion(self) ->DataIngestionArtifact:\\n        \"\"\"\\n        Method Name :   initiate_data_ingestion\\n        Description :   This method initiates the data ingestion components of training pipeline \\n        \\n        Output      :   train set and test set are returned as the artifacts of data ingestion components\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered initiate_data_ingestion method of Data_Ingestion class\")\\n\\n        try:\\n            dataframe = self.export_data_into_feature_store()\\n\\n            logging.info(\"Got the data from mongodb\")\\n\\n            self.split_data_as_train_test(dataframe)\\n\\n            logging.info(\"Performed train test split on the dataset\")\\n\\n            logging.info(\\n                \"Exited initiate_data_ingestion method of Data_Ingestion class\"\\n            )\\n\\n            data_ingestion_artifact = DataIngestionArtifact(trained_file_path=self.data_ingestion_config.training_file_path,\\n            test_file_path=self.data_ingestion_config.testing_file_path)\\n            \\n            logging.info(f\"Data ingestion artifact: {data_ingestion_artifact}\")\\n            return data_ingestion_artifact\\n        except Exception as e:\\n            raise USvisaException(e, sys) from e', metadata={'source': 'test_repo1/us_visa/components/data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/components/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/components/model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/components/model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/components/data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/components/data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/utils/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\nimport numpy as np\\nimport dill\\nimport yaml\\nfrom pandas import DataFrame\\n\\nfrom us_visa.exception import USvisaException\\nfrom us_visa.logger import logging\\n\\n\\ndef read_yaml_file(file_path: str) -> dict:\\n    try:\\n        with open(file_path, \"rb\") as yaml_file:\\n            return yaml.safe_load(yaml_file)\\n\\n    except Exception as e:\\n        raise USvisaException(e, sys) from e\\n\\n\\ndef write_yaml_file(file_path: str, content: object, replace: bool = False) -> None:\\n    try:\\n        if replace:\\n            if os.path.exists(file_path):\\n                os.remove(file_path)\\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n        with open(file_path, \"w\") as file:\\n            yaml.dump(content, file)\\n    except Exception as e:\\n        raise USvisaException(e, sys) from e\\n\\n\\ndef load_object(file_path: str) -> object:\\n    logging.info(\"Entered the load_object method of utils\")\\n\\n    try:\\n\\n        with open(file_path, \"rb\") as file_obj:\\n            obj = dill.load(file_obj)\\n\\n        logging.info(\"Exited the load_object method of utils\")\\n\\n        return obj\\n\\n    except Exception as e:\\n        raise USvisaException(e, sys) from e\\n\\ndef save_numpy_array_data(file_path: str, array: np.array):\\n    \"\"\"\\n    Save numpy array data to file\\n    file_path: str location of file to save\\n    array: np.array data to save\\n    \"\"\"\\n    try:\\n        dir_path = os.path.dirname(file_path)\\n        os.makedirs(dir_path, exist_ok=True)\\n        with open(file_path, \\'wb\\') as file_obj:\\n            np.save(file_obj, array)\\n    except Exception as e:\\n        raise USvisaException(e, sys) from e\\n\\n\\ndef load_numpy_array_data(file_path: str) -> np.array:\\n    \"\"\"\\n    load numpy array data from file\\n    file_path: str location of file to load\\n    return: np.array data loaded\\n    \"\"\"\\n    try:\\n        with open(file_path, \\'rb\\') as file_obj:\\n            return np.load(file_obj)\\n    except Exception as e:\\n        raise USvisaException(e, sys) from e\\n\\n\\ndef save_object(file_path: str, obj: object) -> None:\\n    logging.info(\"Entered the save_object method of utils\")\\n\\n    try:\\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n        with open(file_path, \"wb\") as file_obj:\\n            dill.dump(obj, file_obj)\\n\\n        logging.info(\"Exited the save_object method of utils\")\\n\\n    except Exception as e:\\n        raise USvisaException(e, sys) from e\\n\\n\\ndef drop_columns(df: DataFrame, cols: list)-> DataFrame:\\n\\n    \"\"\"\\n    drop the columns form a pandas DataFrame\\n    df: pandas DataFrame\\n    cols: list of columns to be dropped\\n    \"\"\"\\n    logging.info(\"Entered drop_columns methon of utils\")\\n\\n    try:\\n        df = df.drop(columns=cols, axis=1)\\n\\n        logging.info(\"Exited the drop_columns method of utils\")\\n        \\n        return df\\n    except Exception as e:\\n        raise USvisaException(e, sys) from e', metadata={'source': 'test_repo1/us_visa/utils/main_utils.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#  logger file will track of all bug happenend in the development time and save them inside logs folder\\n# let suppose morining time we got 1 bug \\n# and in evening time we again got 1 bug  \\n# then using log file we can check what was morning bug\\n\\nimport logging\\nimport os\\nfrom from_root import from_root\\nfrom datetime import datetime\\nLOG_FILE = f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log \"\\nlog_dir = \"logs\"\\nlogs_path = os.path.join(from_root(),log_dir,LOG_FILE)\\nos.makedirs(log_dir,exist_ok=True)\\n\\nlogging.basicConfig(\\n    filename=logs_path,\\n    format=\"[%(asctime)s] %(name)s - %(levelname)s - %(message)s\",\\n    level=logging.DEBUG\\n)', metadata={'source': 'test_repo1/us_visa/logger/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\n@dataclass\\nclass DataIngestionArtifact:\\n    trained_file_path:str \\n    test_file_path:str \\n@dataclass\\nclass DataValidationArtifact:\\n    validation_status:bool\\n    message: str\\n    drift_report_file_path: str\\n\\n\\n@dataclass\\nclass DataTransformationArtifact:\\n    transformed_object_file_path:str \\n    transformed_train_file_path:str\\n    transformed_test_file_path:str\\n\\n\\n@dataclass\\nclass ClassificationMetricArtifact:\\n    f1_score:float\\n    precision_score:float\\n    recall_score:float\\n\\n@dataclass\\nclass ModelTrainerArtifact:\\n    trained_model_file_path:str \\n    metric_artifact:ClassificationMetricArtifact\\n\\n@dataclass\\nclass ModelEvaluationArtifact:\\n    is_model_accepted:bool\\n    changed_accuracy:float\\n    s3_model_path:str \\n    trained_model_path:str\\n@dataclass\\nclass ModelPusherArtifact:\\n    bucket_name:str\\n    s3_model_path:str\\n ', metadata={'source': 'test_repo1/us_visa/entity/artifact_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom us_visa.constants import *\\nfrom dataclasses import dataclass\\nfrom datetime import datetime\\n\\nTIMESTAMP: str = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\\n\\n@dataclass\\nclass TrainingPipelineConfig:\\n    pipeline_name: str = PIPELINE_NAME\\n    artifact_dir: str = os.path.join(ARTIFACT_DIR, TIMESTAMP)\\n    timestamp: str = TIMESTAMP\\n\\n\\ntraining_pipeline_config: TrainingPipelineConfig = TrainingPipelineConfig()\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    data_ingestion_dir: str = os.path.join(training_pipeline_config.artifact_dir, DATA_INGESTION_DIR_NAME)\\n    feature_store_file_path: str = os.path.join(data_ingestion_dir, DATA_INGESTION_FEATURE_STORE_DIR, FILE_NAME)\\n    training_file_path: str = os.path.join(data_ingestion_dir, DATA_INGESTION_INGESTED_DIR, TRAIN_FILE_NAME)\\n    testing_file_path: str = os.path.join(data_ingestion_dir, DATA_INGESTION_INGESTED_DIR, TEST_FILE_NAME)\\n    train_test_split_ratio: float = DATA_INGESTION_TRAIN_TEST_SPLIT_RATIO\\n    collection_name:str = DATA_INGESTION_COLLECTION_NAME\\n\\n\\n\\n@dataclass\\nclass DataValidationConfig:\\n    data_validation_dir: str = os.path.join(training_pipeline_config.artifact_dir, DATA_VALIDATION_DIR_NAME)\\n    drift_report_file_path: str = os.path.join(data_validation_dir, DATA_VALIDATION_DRIFT_REPORT_DIR,\\n                                               DATA_VALIDATION_DRIFT_REPORT_FILE_NAME)\\n    \\n\\n\\n\\n@dataclass\\nclass DataTransformationConfig:\\n    data_transformation_dir: str = os.path.join(training_pipeline_config.artifact_dir, DATA_TRANSFORMATION_DIR_NAME)\\n    transformed_train_file_path: str = os.path.join(data_transformation_dir, DATA_TRANSFORMATION_TRANSFORMED_DATA_DIR,\\n                                                    TRAIN_FILE_NAME.replace(\"csv\", \"npy\"))\\n    transformed_test_file_path: str = os.path.join(data_transformation_dir, DATA_TRANSFORMATION_TRANSFORMED_DATA_DIR,\\n                                                   TEST_FILE_NAME.replace(\"csv\", \"npy\"))\\n    transformed_object_file_path: str = os.path.join(data_transformation_dir,\\n                                                     DATA_TRANSFORMATION_TRANSFORMED_OBJECT_DIR,\\n                                                     PREPROCSSING_OBJECT_FILE_NAME)\\n    \\n\\n\\n\\n@dataclass\\nclass ModelTrainerConfig:\\n    model_trainer_dir: str = os.path.join(training_pipeline_config.artifact_dir, MODEL_TRAINER_DIR_NAME)\\n    trained_model_file_path: str = os.path.join(model_trainer_dir, MODEL_TRAINER_TRAINED_MODEL_DIR, MODEL_FILE_NAME)\\n    expected_accuracy: float = MODEL_TRAINER_EXPECTED_SCORE\\n    model_config_file_path: str = MODEL_TRAINER_MODEL_CONFIG_FILE_PATH\\n\\n\\n\\n@dataclass\\nclass ModelEvaluationConfig:\\n    changed_threshold_score: float = MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE\\n    bucket_name: str = MODEL_BUCKET_NAME\\n    s3_model_key_path: str = MODEL_FILE_NAME\\n\\n\\n\\n@dataclass\\nclass ModelPusherConfig:\\n    bucket_name: str = MODEL_BUCKET_NAME\\n    s3_model_key_path: str = MODEL_FILE_NAME\\n\\n\\n\\n@dataclass\\nclass USvisaPredictorConfig:\\n    model_file_path: str = MODEL_FILE_NAME\\n    model_bucket_name: str = MODEL_BUCKET_NAME\\n', metadata={'source': 'test_repo1/us_visa/entity/config_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/entity/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\n\\nfrom us_visa.exception import USvisaException\\nfrom us_visa.logger import logging\\n\\nimport os\\nfrom us_visa.constants import DATABASE_NAME, MONGODB_URL_KEY\\nimport pymongo\\nimport certifi\\n\\nca = certifi.where()\\n\\nclass MongoDBClient:\\n    \"\"\"\\n    Class Name :   export_data_into_feature_store\\n    Description :   This method exports the dataframe from mongodb feature store as dataframe \\n    \\n    Output      :   connection to mongodb database\\n    On Failure  :   raises an exception\\n    \"\"\"\\n    client = None\\n\\n    def __init__(self, database_name=DATABASE_NAME) -> None:\\n        try:\\n            if MongoDBClient.client is None:\\n                mongo_db_url = os.getenv(MONGODB_URL_KEY)\\n                if mongo_db_url is None:\\n                    raise Exception(f\"Environment key: {MONGODB_URL_KEY} is not set.\")\\n                MongoDBClient.client = pymongo.MongoClient(mongo_db_url, tlsCAFile=ca)\\n            self.client = MongoDBClient.client\\n            self.database = self.client[database_name]\\n            self.database_name = database_name\\n            logging.info(\"MongoDB connection succesfull\")\\n        except Exception as e:\\n            raise USvisaException(e,sys)', metadata={'source': 'test_repo1/us_visa/configuration/mongo_db_connection.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/configuration/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nfrom us_visa.exception import USvisaException\\nfrom us_visa.logger import logging\\n\\nfrom us_visa.components.data_ingestion import DataIngestion\\nfrom us_visa.components.data_validation import DataValidation\\nfrom us_visa.components.data_transformation import DataTransformation\\nfrom us_visa.components.model_trainer import ModelTrainer\\nfrom us_visa.components.model_evaluation import ModelEvaluation\\nfrom us_visa.components.model_pusher import ModelPusher\\n\\nfrom us_visa.entity.config_entity import (DataIngestionConfig,\\n                                          DataValidationConfig,\\n                                          DataTransformationConfig,\\n                                          ModelTrainerConfig,\\n                                          ModelEvaluationConfig,\\n                                          ModelPusherConfig)\\n                                          \\n\\nfrom us_visa.entity.artifact_entity import (DataIngestionArtifact,\\n                                            DataValidationArtifact,\\n                                            DataTransformationArtifact,\\n                                            ModelTrainerArtifact,\\n                                            ModelEvaluationArtifact,\\n                                            ModelPusherArtifact)\\n\\n\\n\\nclass TrainPipeline:\\n    def __init__(self):\\n        self.data_ingestion_config = DataIngestionConfig()\\n        self.data_validation_config = DataValidationConfig()\\n        self.data_transformation_config = DataTransformationConfig()\\n        self.model_trainer_config = ModelTrainerConfig()\\n        self.model_evaluation_config = ModelEvaluationConfig()\\n        self.model_pusher_config = ModelPusherConfig()\\n\\n\\n    \\n    def start_data_ingestion(self) -> DataIngestionArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting data ingestion component\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the start_data_ingestion method of TrainPipeline class\")\\n            logging.info(\"Getting the data from mongodb\")\\n            data_ingestion = DataIngestion(data_ingestion_config=self.data_ingestion_config)\\n            data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\\n            logging.info(\"Got the train_set and test_set from mongodb\")\\n            logging.info(\\n                \"Exited the start_data_ingestion method of TrainPipeline class\"\\n            )\\n            return data_ingestion_artifact\\n        except Exception as e:\\n            raise USvisaException(e, sys) from e\\n        \\n    \\n\\n    def start_data_validation(self, data_ingestion_artifact: DataIngestionArtifact) -> DataValidationArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting data validation component\\n        \"\"\"\\n        logging.info(\"Entered the start_data_validation method of TrainPipeline class\")\\n\\n        try:\\n            data_validation = DataValidation(data_ingestion_artifact=data_ingestion_artifact,\\n                                             data_validation_config=self.data_validation_config\\n                                             )\\n\\n            data_validation_artifact = data_validation.initiate_data_validation()\\n\\n            logging.info(\"Performed the data validation operation\")\\n\\n            logging.info(\\n                \"Exited the start_data_validation method of TrainPipeline class\"\\n            )\\n\\n            return data_validation_artifact\\n\\n        except Exception as e:\\n            raise USvisaException(e, sys) from e\\n        \\n\\n\\n    \\n\\n    def start_data_transformation(self, data_ingestion_artifact: DataIngestionArtifact, data_validation_artifact: DataValidationArtifact) -> DataTransformationArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting data transformation component\\n        \"\"\"\\n        try:\\n            data_transformation = DataTransformation(data_ingestion_artifact=data_ingestion_artifact,\\n                                                     data_transformation_config=self.data_transformation_config,\\n                                                     data_validation_artifact=data_validation_artifact)\\n            data_transformation_artifact = data_transformation.initiate_data_transformation()\\n            return data_transformation_artifact\\n        except Exception as e:\\n            raise USvisaException(e, sys)\\n        \\n\\n    \\n    def start_model_trainer(self, data_transformation_artifact: DataTransformationArtifact) -> ModelTrainerArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting model training\\n        \"\"\"\\n        try:\\n            model_trainer = ModelTrainer(data_transformation_artifact=data_transformation_artifact,\\n                                         model_trainer_config=self.model_trainer_config\\n                                         )\\n            model_trainer_artifact = model_trainer.initiate_model_trainer()\\n            return model_trainer_artifact\\n\\n        except Exception as e:\\n            raise USvisaException(e, sys)\\n        \\n    \\n\\n    def start_model_evaluation(self, data_ingestion_artifact: DataIngestionArtifact,\\n                               model_trainer_artifact: ModelTrainerArtifact) -> ModelEvaluationArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting modle evaluation\\n        \"\"\"\\n        try:\\n            model_evaluation = ModelEvaluation(model_eval_config=self.model_evaluation_config,\\n                                               data_ingestion_artifact=data_ingestion_artifact,\\n                                               model_trainer_artifact=model_trainer_artifact)\\n            model_evaluation_artifact = model_evaluation.initiate_model_evaluation()\\n            return model_evaluation_artifact\\n        except Exception as e:\\n            raise USvisaException(e, sys)\\n        \\n\\n    \\n\\n    def start_model_pusher(self, model_evaluation_artifact: ModelEvaluationArtifact) -> ModelPusherArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting model pushing\\n        \"\"\"\\n        try:\\n            model_pusher = ModelPusher(model_evaluation_artifact=model_evaluation_artifact,\\n                                       model_pusher_config=self.model_pusher_config\\n                                       )\\n            model_pusher_artifact = model_pusher.initiate_model_pusher()\\n            return model_pusher_artifact\\n        except Exception as e:\\n            raise USvisaException(e, sys)\\n\\n        \\n\\n    \\n\\n        \\n\\n    \\n    def run_pipeline(self, ) -> None:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for running complete pipeline\\n        \"\"\"\\n        try:\\n            data_ingestion_artifact = self.start_data_ingestion()\\n            data_validation_artifact = self.start_data_validation(data_ingestion_artifact=data_ingestion_artifact)\\n            data_transformation_artifact = self.start_data_transformation(\\n                data_ingestion_artifact=data_ingestion_artifact, data_validation_artifact=data_validation_artifact)\\n            model_trainer_artifact = self.start_model_trainer(data_transformation_artifact=data_transformation_artifact)\\n            model_evaluation_artifact = self.start_model_evaluation(data_ingestion_artifact=data_ingestion_artifact,\\n                                                                    model_trainer_artifact=model_trainer_artifact)\\n            \\n            if not model_evaluation_artifact.is_model_accepted:\\n                logging.info(f\"Model not accepted.\")\\n                return None\\n            model_pusher_artifact = self.start_model_pusher(model_evaluation_artifact=model_evaluation_artifact)\\n\\n\\n        \\n        except Exception as e:\\n            raise USvisaException(e, sys)', metadata={'source': 'test_repo1/us_visa/pipeline/training_pipeline.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/pipeline/__init__.py.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo1/us_visa/pipeline/prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunkings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "chunk_size=2000,\n",
    "chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
